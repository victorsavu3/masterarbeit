\chapter{Background}

\section {Parallelization}

\section {Dynamic Binary Instrumentation}

\subsection{Introduction}

Dynamic binary instrumentation is a method to analyze and alter the dynamic behavior of an application by modifying its instructions at runtime. This method makes it possible to gain insight into the state of a running program at any point in time and can also allow tools to alter that state. Dynamic binary instrumentation is used to follow a specific path trough an applications and is by no means exhaustive.

Most tools developed using this method are designed to obtain information about an application. For example tools such as MemTrace \cite{pindoc} and Memcheck \cite{memcheck} help discover and correct memory problems. Intel Advisor \cite{inteladvisor} and the tools described in this thesis aid in parallelization endevours.

Dynamic binary instrumentation can also be used to alter the state of a running program. Tools have implemented automatic fault injection \cite{faultinject}, dynamic transactional memory \cite{dynamicstm} and improvements to security \cite{dynamicstackprotect}.

\subsection{Implementations}

\subsubsection{Valgrind}

Valgrind \cite{valgrind} is an open-source dynamic binary instrumentation framework that is used to create a number of powerful and diverse tools. It focuses on using shadow values to implement complex tools, that are difficult to develop with other frameworks. Unfortunately simple tools need a complex implementation and have a considerable overhead.

\subsubsection{Dyninst}

Dyninst \cite{dyninst} is a framework developed at the University of Maryland that performs dynamic binary instrumentation on a target application. The goal of this project is to abstract away machine code in order to create simple and portable tools. Multiple projects have been implemented using Dyninst such as VampirTrace \cite{vampirtrace}.

\subsubsection{Intel Pin}

Intel Pin \cite{pin} is a instrumentation framework that can be used to build a large variety of tools for Windows and Linux. The implementation is focused on ease-of-use and provides a very simple but powerful API that allows tools to instrument any instruction in a program.

\subsubsection{Example tools}

\emph{Memcheck} \cite{memcheck} is a tool developed as part of Valgrind that detects common C and C++ memory errors. It is able to report accesses to unallocated or uninitialized memory, incorrect usage of allocations or deallocations and memory leaks.

\emph{Intel Advisor} \cite{inteladvisor} is a collection of related tools based on Intel Pin that focus on helping developers parallelize applications. It is able to predict possible improvements when applying vectorisation and thread parallelism to applications.

\subsection{Overhead}

Unfortunately dynamic binary instrumentation entails a performance overhead when running programs. Valgrind for example runs programs 4.3x slower \cite{valgrind} even when not performing any kind of analysis. The slowdown becomes more substantial (22.1x) when executing a proper tool.

The overhead of the instrumentation can be split up in tow parts, modification of the target program and the execution of additional instructions. Tools such as Valgrind and Intel Pin also utilize a just-in-time virtual machine to execute the target, which creates an additional overhead. In \cite{instoverhead} there is a breakdown of time and instruction count when running an application with Intel Pin and performing an analysis.

\subsection{Alternatives}

Static analysis tools such as the Clang Static Analyser \cite{clang} or Coverity \cite{coverity} are able to identify potential faults in an application by inspecting its source code. This type of scrutiny considers all possible paths an application can potentially follow. This will detect problems in code that is rarely tested, but can also make it very computationally intensive. In contrast to this dynamic analysis only follows a single path trough the application, but it can find many more issues with this one particular execution.

Code instrumentation tools such as the Intel Advisor \cite{inteladvisor} allow a software developer to insert instrumentations into the source code of the application. Running analyses on the application becomes a cycle of editing the source code, rebuilding and then running. This cycle can be too restrictive when working with an existing application that depends on a convoluted build system. Binary instrumentation works with the compiled application directly and does not require the developer to perform rebuilds.

Static binary instrumentation tools such as PEBIL \cite{pebil} alter an application executable by changing the instructions contained within. The new program can then be executed normally. This method can provide some performance benefits, but not in all cases as can be seen in \cite{pebilperf}. The drawback of these tools is the lack of flexibility when instrumenting as it is not possible to change instructions at runtime.

\subsection{Intel Pin}

The implementation of the tools presented in this thesis ares based on Intel Pin. This framework has been chosen because of the simple API, performance, good documentation and the flexibility when performing instrumentation.

\subsubsection{API}

The Pin API \cite{pindoc} simplifies the development of tools by providing two types of callbacks. Instrumentation routines are called when the tools must process parts of the target program. Analysis routines are the instrumentations that are called during the program execution. All routines are developed using standard C/C++ and can also take advantage of any existing library.

Two of the most interesting instrumentation routines are trace callbacks and image callbacks. A image callback is called when the application or a shared library is loaded and allow the tool to instrument functions as they are detected in the executable code. A trace callback is called when the JIT compiler encounters new code and allows the tool to insert instrumentations into the instruction stream.

Instrumentation routines insert calls to analysis routines in order to acquire information or to modify the program state. Pin attempts to reduce the overhead of inserted code by automatically optimizing the analysis routines and the calling site.

The provided API allows a tool to perform a very granular analysis without the need to go trough a intermediate representation of the instruction stream. This greatly simplifies development and makes it possible to optimize the instrumentation.

\subsubsection{Documentation}

Intel Pin provides a comprehensive documentation \cite{pindoc} and a large number of examples as part of its distribution. The provided samples illustrate the different classes of tools that can be developed using the framework.

\subsubsection{Fast Buffers}

The Intel Pin Fast Buffering API \cite{pinbuffer} aims to decouple information collection from processing in order to reduce the overhead associated with dynamic binary instrumentation. This is implemented by collecting multiple chunks of data and processing them only when the buffer used to store them is full.

The greatest benefit of using the Intel Pin Fast Buffering API is the increased performance when performing analyses on applications \cite{pinbuffer}. This comes at the cost of being unable to alter the application state during execution. The tools developed as part of this thesis need to change the application behavior and are implemented using this API.

The provided API is similar to the standard Pin API \cite{pindoc}. Instrumentation routines remain unchanged, but it is not possible to insert instrumentations into the program outside trace instrumentation. Buffer allocation and management is performed automatically by Pin and the tool only needs to define a buffer with a callback to handle the data processing. Information gathering is performed by instrumentation code generated by Pin ans can examine any aspect of the application state.

\section{SQLite}

SQLite \cite{sqlitebook} is an open-source relational database management system that does not require a server. It is implemented as a self-contained library that supports most features of the SQL92 standard.

In contrast to traditional RDBMS, SQLite is not based on a client/server architecture. Its entire engine is integrated into the application that accesses or manipulates the database. This reduces the footprint and complexity of a system that incorporates SQLite instead of a server.

SQLite stores the entire database in a single file. This file contains the schema, all data stored in tables and indexes. The storage format is platform agnostic which allows easy backup and sharing of an entire database.

TODO Performance

The tools implemented as part of this thesis store data as a SQLite database. This allows results of an application trace to be easily manipulated, while maintaining a high degree of performance. Additional analysis can be easily performed on the database by using standard SQL statements, as will be presented in Section \ref{parceiveui}.

\section {Visualizations}

Gathering large amounts of data about an applications does benefit any user if it can not be navigated or condensed into an useful form. A graphic visual representation of the data makes it possible to derive useful knowledge about a program.

\subsection {SVG}

SVG \cite{svg11} is a vector image format based on XML designed for two-dimensional graphics based on a open Web standard. It can be easily integrated into HTML and supports interactivity and complex animations. Since 2011 most browsers implement native support for SVG.

Many visualization frameworks have been developed to work with SVG inside browsers such as D3, Vega, Processing or dygraphs. The standard provides all primitives required to implement any kind of 2D visualization. Interactivity and animations are possible with javascript as SVG documents are part of the Document Object Model.

The most important features of SVG associated with visualizations are:

\begin{itemize}
	\item[Paths] allow arbitrary shapes to be defined
	\item[Basic shapes] such as lines, circles and rectangles
	\item[Text elements] makes it easy to insert text in a visualization
	\item[Groups] allows easy and efficient manipulation of multiple elements
	\item[Interactivity] implemented with the use of events and the DOM
	\item[Animations] possible with CSS, Javascript or built-in elements
	\item[Linking] allows references to other parts of the document
\end{itemize}

\subsection {Javascript}

\section{Parceive}

Parceive \cite{parceive} is a tracing-based tool for parallelizing existing sequential software. It focuses on high-level architecture analysis, but also gathers fine-grained memory accesses and call information.

\subsection{Architecture}

In Figure \ref{parceive:architecture} we can see the overall architecture of the Parceive project. Parceive is based on Intel Pin and is designed to store a trace of the application in a SQLite database. This database is then processed to improve the performance of views that access information from it. A simple NodeJS is implementet as a bridge between the browser and the database. Views access data using a Object Relational Mapping that performs multiple optimizations to improve throughput. The view framework organizes the display of views on the page and implements state management.

\subsection{Pin tool}

Parceive is the data gathering component of the project. It traces the run of an application and stores its the function calls, memory accesses and loop executions in a SQLite database. The size of the result can be reduced by applying filters at the image, source file and function level.

This tool also performs variable name resolution using DWARF or DebugHelp information present in the executable. This greatly improves the quality of the information available to visualizations.

\subsection{Database layout}

The database layout used by Parceive is closely related to the entities available in the Pin API and is very simple as can be see in Figure \ref{parceive:layout}. In this section we will explain what each table contains and how entities are related to each other.

An Image is the program executable or a shared library loaded by it and can contain multiple functions and source files. The Call Table contains information about all the unfiltered function calls made by the application. A call consists of multiple segments which contain actual instructions. Segments are used to indicate loops inside calls. If a segment is part of a loop a corresponding LoopIteration entry exists to store details. Loop iterations are grouped into loop executions so the different loops can be identified. Static information about loops is stored in the Loop table

Instructions are only stored when they access memory, call a function, create a thread or allocate memory. A instruction can access multiple memeory locations and in different ways. The Reference table contains interesting memory slices and is populated using debug information or allocations. Currently arrays are stored as one single reference.

\begin{sidewaysfigure}
	\centering
	\includegraphics[width=1\textwidth]{parceive-schema}
	\caption{Parceive database layout}
	\label{parceive:layout}
\end{sidewaysfigure}

\subsection{Database processing}

The database layout and settings used by Parceive are highly optimized for fast writing. All the information required by the visualization can be obtained by using queries, but most of the operations will take a considerable amount of time to complete.

Since no more data will be inserted into the database storing redundant information is not a problem. 

\subsubsection{Examples}

\subsubsection{Final layout}

\begin{sidewaysfigure}
	\centering
	\includegraphics[width=0.8\textwidth]{full-schema}
	\caption{Processed database layout}
	\label{parceive:proclayout}
\end{sidewaysfigure}

\subsection{ORM}

\subsection{View Framework}

\begin{sidewaysfigure}
	\begin{tikzpicture}[auto, scale=0.6, node distance=1.5cm,>=latex']
	\tikzset{block/.style= {draw, scale=1, rectangle, align=center,minimum height=1cm}}
	\tikzset{blockred/.style= {line width=1.0pt, draw, scale=1, rectangle, draw=blue, align=center,minimum height=1cm}}
	\tikzset{browserframe/.style= {thick, draw=blue, dotted,inner sep=0.2em}}
	\tikzset{above/.style= {scale=0.6}}
	\tikzset{free/.style= {align=center,scale=0.8}}			
	
	\node [blockred] (nodejs) {NodeJS Server};
	\node [block, left = of nodejs] (database) {Database};
	\node [block, left = of database] (parceive) {Parceive};
	\node [blockred, below = of database] (processing) {Processing};
	\node [block, left = of parceive] (application) {Application};
	\node [blockred, below = of nodejs] (orm) {ORM};
	\node [blockred, right = of orm] (framework) {View Framework};
	\node [blockred, below = of orm] (cct) {CCT View};
	\node [block, below = of framework] (profiling) {Profiling View};
	\node [browserframe,fit=(orm)(framework)(cct)(profiling)] (browser) {};
	\node [free, above of = browser, yshift=1.5cm]{Browser};			
	
	\draw[->] (parceive) -- node[above] {traces} (application);
	\draw[->] (parceive) -- node[above] {generates} (database);
	\draw[<->] (database) -- (processing);
	\draw[->] (nodejs) -- node[above] {reads} (database);
	\draw[->] (orm) -- node[above] {uses} (nodejs);
	\draw[->] (cct) -- (orm);
	\draw[->] (cct) -- (framework);
	\draw[->] (profiling) -- (orm);
	\draw[->] (profiling) -- (framework);
	\end{tikzpicture}
	
	\caption{Parceive architecture}
	\label{parceive:architecture}
\end{sidewaysfigure}