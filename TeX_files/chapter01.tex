\chapter{Introduction}

\section {Motivation}

Single core performance in CPUs has stalled for the last 10 years as processor manufacturers focus on multi-core systems in order to increase performance \cite{procspeed2}. Unfortunately, most of the existing software can not take advantage of the increasing processing speed, because they operate sequentially. Since replacing hardware will not have as much of an impact as it had in the past \cite{procspeed}, scaling existing systems to meet new requirements becomes more difficult.

Parallelizing software is a daunting proposition as software architectures have a purely sequential design. The programmers involved need to identify parts of their application that consume the most processing time and decide whether it is actually expedient to parallelize them.

This thesis presents a framework with two tools developed using Intel Pin \cite{pin} that use dynamic binary instrumentation to obtain information about a application. In order to utilize this data multiple visualizations were developed that help the user in determining which parts of an application can be parallelized. The framework can be easily extended to gather more information that can be displayed in tailored visualizations.

The framework is based on the concept of tagging sections of a programs source code. This is done interactively or can be inferred automatically by another tool. The tags are then applied to the program when it is instrumented. Changes to the source code of the applications are not required for this tool to function because tags are stored in a separate configuration file. This makes instrumenting a painless process on complex systems or when dealing with convoluted build processes.

In order to illustrate the usefulness of tools in the analysis of existing software we will present a simple example program in Figure \ref{cap1:emsim:seq}, which needs to be parallelized. The file is part of a simulator for the UEFA European Championship that was implemented sequentially.

\begin{figure}
	\begin{center}
		\inputminted[linenos, fontsize=\scriptsize]{c}{emsim_seq.c}
	\end{center}
	\caption{\texttt{emsim\_seq.c}: EM Simulator sequential implementation}
	\label{cap1:emsim:seq}
\end{figure}

Profiling indicates that the loops present in this code take up almost all the execution time as can be seen in Figure \ref{cap1:emsim:profile}. Loops in have a high potential for parallelism, but it can only be achieved if there are no dependencies between the iterations. Unfortunately it is difficult to analyze such dependencies. For example, the pointer arithmetic present in the loop starting in line 12 makes source code inspection very challenging, without understanding the logic behind the implementation it becomes impossible. Generally, it is difficult to grasp the dependencies without a thorough understanding of the application and careful analysis. 

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.8\textwidth]{emsimprofile}
	\caption{Profile view of the execution the code using Parceive}
	\label{cap1:emsim:profile}
\end{figure}

One of our visualizations is able to simulate the speed up when running sections of code in parallel to each other and to show the memory dependencies between the sections. Visualizations of loops on lines 12 and 30 in Figures \ref{cap1:emsim:sections1} and \ref{cap1:emsim:sections2} respectively show the possible speedup when running on a machine with 4 threads and more importantly the actual conflicts that the user must resolve in order to maintain correctness. The loop on line 12 can be easily modified to run in separate threads, even though all iterations work on the same arrays, since iterations works on distinct elements.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.8\textwidth]{loop1section}
	\caption{Section view of the loop on line 12 in Figure \ref{cap1:emsim:seq}}
	\label{cap1:emsim:sections1}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.8\textwidth]{loop2section}
	\caption{Section view of the loop on line 30 in Figure \ref{cap1:emsim:seq}}
	\label{cap1:emsim:sections2}
\end{figure}

As can be seen in these examples, information about the program can be gained by our tools. Profiling is a well known method of finding performance hot-spots in applications. In this thesis we propose tools that can help determine the potential for parallelization of these sections of code by simulating their parallel execution. This gives the user information about the possible speedup and the conflicts that require the code to remain sequential.

\section {Contribution}

This thesis presents a framework for gathering information about applications using dynamic binary instrumentation and tagging of the source code. Two analyses have been developed to help users when parallelizing existing applications, but more can be easily developed.

The first is section analysis, which simulates running parts of the source code in parallel. In Figures \ref{cap1:emsim:sections1} and \ref{cap1:emsim:sections2} the loops are tagged as sections and each iteration starts a new task. The visualization presents the possible performance benefit and conflicts when running the tasks within a section in parallel.

Our framework can also perform pipeline analysis to cover a different use case. Pipeline parallelism can be be applied to some loops, despite the fact that iterations can not be run in parallel to each other. This approach splits each iteration into stages and different stages from different iterations can be run parallel to each other. As a simple example we consider file processing, where data is read, processed and written to another file. Individual iterations can not be run in parallel, but the io operations and the processing can be seen in Figure \ref{cap1:pipeline}. In Chapter ยง2 we will present a more concrete example, bzip compression.

\begin{figure}[!ht]
	\centering
	\begin{tabular}{ l | l | l | l | l}
		R & P & W & & \\
		\hline
		R &  & P & W & \\
		\hline
		R &  & & P & W \\
	\end{tabular}
	\caption{Pipeline, R - read, P - proecess, W - write}
	\label{cap1:pipeline}
\end{figure}

\section {Related Work}

\subsection{Profiling and Tracing}

The tools presented in this thesis are able to gather tracing information about the application, but it is imprecise and slow in this regard. As such it is recommended to use an existing profiler or tracer to discover the performance bottlenecks. Our framework becomes useful after this step, as it helps the user to determine how to eliminate the bottlenecks.

\subsection{Parceive}

Parceive \cite{parceive} is a tracing-based tool for parallelizing existing sequential software. It focuses on high-level architecture analysis, but also gathers fine-grained memory accesses and call information.

The implementation of this tool generates a database compatible with Parceive, keeping all of its existing views functional. The biggest difference is that additional information about tags is recorded, making our analysis possible. The tracing can also be controlled using tags, making it possible for large applications to be analyzed.

We have also created multiple new views that are designed to help a developer in analyzing parallelization opportunities. These visualizations are based on the framework provided by the Parceive UI.

\subsection{Intel Advisor}

Intel Advisor \cite{inteladvisor} is a collection of related tools that focus on helping developers parallelize applications to take advantage of modern processors. It focuses on vectorization and thread parallelism.

The section analysis that our framework provides is also possible in Intel Advisor, but with a different workflow. The source code of the application needs to be annotated using macros provided by the tool such as \texttt{ANNOTATE\_SITE\_BEGIN} or \texttt{ANNOTATE\_TASK\_BEGIN}. This requires rebuilding the application and then instrumenting it using Intel Pin. Our tool is able to instrument the application without the need for source code changes, reducing the number of steps required to gain information about the application.

Intel Advisor offers many features not available in our tool such as vectorization and memory access pattern analysis. We expect the two tools to complement each other when trying to achieve the greatest performance gains.

\section {Example}

To better illustrate the use of our tools we will go trough a simple example that will be parallelized. The relevant code is shown in Figure \ref{cap1:emsim:seq}.

\subsection {Static analysis}

The first step a user must perform is the static analysis. This extracts all locations in the source code where a tag can be inserted. The application itself is executed in order to analyze all dynamic libraries that it uses.

\subsection{Execution}

\begin{lstlisting}[style=BashInputStyle]
pin.sh -t libpintool\_static.so -db emsim.db -filter filter.yaml \
  -- emsim_seq
\end{lstlisting}

This command starts pin and loads \texttt{libpintool\_static.so} as a tool. The result is saved in the \textbf{emsim.db} database. The command to start the application that is being instrumented follows \texttt{--}. The filter in Figure \ref{cap1:filter} is used to reduce the amount of work needed to be performed

\begin{figure}
	\begin{center}
		\begin{minted}{yaml}
image:
  exclude:
    - /lib64/*
file:
  exclude:
    - /usr/include
    - Unknown
    - sqlite.c
function:
  exclude:
  - _GLOBAL__*
  - __static_initialization_and_destruction_0
		\end{minted}
	\end{center}
	\caption{\texttt{filter.yaml} example}
	\label{cap1:filter}
\end{figure}

On systems with a more recent kernel it is necessary to supply Pin with the \texttt{-ifeellucky} to avoid some ineffective version checks.

\subsection {Tagging the code}

First the database must be copied to the import folder of \texttt{parceive-ui} and processed:

\begin{lstlisting}[style=BashInputStyle]
gulp db
\end{lstlisting}

Then the UI can be started:

\begin{lstlisting}[style=BashInputStyle]
gulp server
\end{lstlisting}

Figure \ref{cap1:tagview} shows the UI to tag code. Using this interface 2 \textbf{Tags} are defined, \textit{Section 1} and \textit{Section 1 Task}. Three \textbf{TagInstructions} are then defined to delimit the sections of code.

\textit{Section 1} has a \texttt{Start} at line 11 and a \texttt{Stop} at line 22. This encompasses the entire loop.  \textit{Section 1 Task} has only a \texttt{Start} at line 14. Each iteration a new task is started and the old one is automatically closed. Figure \ref{cap1:source} shows the generated \texttt{source.yaml}.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.8\textwidth]{tagging-view}
	\caption{UI for tagging}
	\label{cap1:tagview}
\end{figure}

\begin{figure}
	\begin{center}
		\begin{minted}{json}
{
  "tags": [
    { "name": "Section 1", "type": "Section" },
    { "name": "Section 1 Task", "type": "Task" }
  ],
  "tagInstructions": [
    { "tag": 1, "type": "Start", "location": 365 },
    { "tag": 1, "type": "Stop", "location": 372 },
    { "tag": 2, "type": "Start", "location": 367 }
  ]
}
		\end{minted}
	\end{center}
	\caption{Generated source.yaml}
	\label{cap1:source}
\end{figure}

\subsection {Dynamic analysis}

With \texttt{source.yaml} containing the tagging information it is possible to execute the dynamic analysis:

\begin{lstlisting}[style=BashInputStyle]
pin.sh -t libpintool\_dynamic.so -db emsim.db -filter filter.yaml \ 
  -source source.yaml -- emsim_seq em.db 1
\end{lstlisting}

The database from the static analysis is used for this execution and all the tracing and tag information is now stored within.

\subsection {Existing Visualizations}

The database must be imported again and all existing visualizations in \texttt{parceive-ui} continue to work as can be seen in Figure \ref{cap1:cct} and \ref{cap1:profiling}.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.8\textwidth]{cct}
	\caption{CCT View}
	\label{cap1:cct}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.8\textwidth]{profiling}
	\caption{Profiling View}
	\label{cap1:profiling}
\end{figure}

\subsection {Visualizations}

\section {Outline}

Chapter 2 provides an broad introduction into concepts and implementations that have made this thesis possible and worthwhile.

In chapter 3 we present the implementation of \texttt{pintool\_static.so}, \texttt{pintool\_dynamic.so} and visualizations that display data from specialized analyses. New analyses and visualizations can be easily developed.

Chapter 4 applies our tools to \texttt{cppcheck} \cite{cppcheck} and bzip \cite{bzip} compression. The first will show the utility of the section analysis and the latter of the pipeline analysis.